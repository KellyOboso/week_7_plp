# ===========================================
# AI Ethics Week 7: Fairness Audit Notebook
# COMPAS Dataset Bias Analysis using AI Fairness 360
# ===========================================

# Install required packages (if needed)
# !pip install aif360 pandas matplotlib scikit-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from aif360.datasets import CompasDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# -------------------------------
# 1. Load the COMPAS dataset
# -------------------------------
compas = CompasDataset()

print("COMPAS dataset loaded.")
print(f"Number of instances: {compas.features.shape[0]}")
print(f"Protected attributes: {compas.protected_attribute_names}")

# Define privileged and unprivileged groups
privileged_groups = [{'race': 1}]  # Caucasian
unprivileged_groups = [{'race': 0}]  # African-American

# -------------------------------
# 2. Check for bias
# -------------------------------
metric = BinaryLabelDatasetMetric(compas,
                                  unprivileged_groups=unprivileged_groups,
                                  privileged_groups=privileged_groups)

print("\n=== Fairness Metrics ===")
print(f"Difference in mean outcomes (mean difference): {metric.mean_difference():.4f}")
print(f"Disparate Impact Ratio: {metric.disparate_impact():.4f}")

# -------------------------------
# 3. Visualize the dataset distribution
# -------------------------------
priv_count = np.sum(compas.protected_attributes == 1)
unpriv_count = np.sum(compas.protected_attributes == 0)

plt.bar(['Privileged (Caucasian)', 'Unprivileged (African-American)'],
        [priv_count, unpriv_count],
        color=['blue', 'orange'])
plt.title('Group Counts in COMPAS Dataset')
plt.ylabel('Number of Samples')
plt.show()

# -------------------------------
# 4. Train/Test Split
# -------------------------------
compas_train, compas_test = compas.split([0.7], shuffle=True)

# -------------------------------
# 5. Train a simple classifier
# -------------------------------
X_train = compas_train.features
y_train = compas_train.labels.ravel()

X_test = compas_test.features
y_test = compas_test.labels.ravel()

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print("\nClassifier Accuracy (Original):", accuracy_score(y_test, y_pred))

# Metrics for predictions
pred_test = compas_test.copy()
pred_test.labels = y_pred

classified_metric = ClassificationMetric(
    compas_test, pred_test,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

print("False Positive Rate Difference (Original):",
      classified_metric.false_positive_rate_difference())

print("Equal Opportunity Difference (Original):",
      classified_metric.equal_opportunity_difference())

# -------------------------------
# 6. Mitigate Bias: Reweighing
# -------------------------------
RW = Reweighing(unprivileged_groups=unprivileged_groups,
                privileged_groups=privileged_groups)
compas_train_transf = RW.fit_transform(compas_train)

# Train classifier on reweighted data
lr_rw = LogisticRegression(max_iter=1000)
lr_rw.fit(X_train, y_train, sample_weight=compas_train_transf.instance_weights)

y_pred_rw = lr_rw.predict(X_test)

print("\nClassifier Accuracy (Reweighted):", accuracy_score(y_test, y_pred_rw))

# Metrics for mitigated predictions
pred_test_rw = compas_test.copy()
pred_test_rw.labels = y_pred_rw

classified_metric_rw = ClassificationMetric(
    compas_test, pred_test_rw,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

print("False Positive Rate Difference (Reweighted):",
      classified_metric_rw.false_positive_rate_difference())

print("Equal Opportunity Difference (Reweighted):",
      classified_metric_rw.equal_opportunity_difference())

# -------------------------------
# 7. Visualize Before vs After
# -------------------------------
before = [
    classified_metric.false_positive_rate_difference(),
    classified_metric.equal_opportunity_difference()
]

after = [
    classified_metric_rw.false_positive_rate_difference(),
    classified_metric_rw.equal_opportunity_difference()
]

labels = ['FPR Diff', 'Equal Opp Diff']

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, before, width, label='Original')
rects2 = ax.bar(x + width/2, after, width, label='Reweighted')

ax.set_ylabel('Metric Value')
ax.set_title('Bias Metrics Before and After Mitigation')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

plt.show()

print("\nâœ… Fairness audit complete.")
